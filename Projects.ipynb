{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 1: Web Scraping with BeautifulSoup\n",
    "\n",
    "1. https://github.com/VeenaKGit/Web-Scraper\n",
    "2. Scrape Using Beautiful, Selenium and Scrapy - https://www.youtube.com/watch?v=zucvHSQsKHA \n",
    "2. This project is a standalone file - scrape_cms.py which scrapes CoreyMS website for each blog \"headline\", \"summary\" and \"youTube link\" and write the same to cms_scrape.csv file.\n",
    "3. https://stackoverflow.com/questions/39319070/is-from-flask-import-request-identical-to-import-requests#:~:text=The%20Flask%20request%20object%20contains,response%20from%20the%20external%20site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation\n",
    "Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Beautifusoup4 is required.\n",
    "2. Parser is required to parse the HTML. There are definitely differetn parser availabe and based on the kind of HTML we are parsing different parser will fll in the \"missing fields\" differently. Because not all websites are perfect.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers\n",
    "3. lxml is throwing some error so using html5lib.\n",
    "    * https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample html for demo\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# this code may not work if bs4 is not installed in this path.\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser') # html.parser is the default parder available in python \n",
    "print(soup.prettify()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "title\n",
      "The Dormouse's story\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.text) # soup.title.string both are same\n",
    "print(soup.title.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "['title']\n",
      "['title']\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
     ]
    }
   ],
   "source": [
    "print(soup.p) # gives the first '<p>' tag in the body\n",
    "print(soup.p['class']) # WE can get the Attributes values in a tag by accessing it as a dict key\n",
    "print(soup.p.get('class')) # same as above, better use this method.\n",
    "print(soup.a)\n",
    "print(soup.find_all('a')) # gives a list of all <a> tags\n",
    "print(soup.find(id='link3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/elsie\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "http://example.com/tillie\n"
     ]
    }
   ],
   "source": [
    "# One common task is extracting all the URLs found within a page’s <a> tags:\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another common task is extracting all the text from a page:\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to Pivotal Cloud Factory:\n",
    "Cloud Factory is the Company and Pivotal is the cloud service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.youtube.com/watch?v=IxXdJf69E3k 1:49:00 onwards.\n",
    "2. Make sure first that your Flask/Django is working on your local server first. \n",
    "2. Create a account on Pivotal Login Page. Created Email:veena.kalburgi@gmail.com Pass:Nice_Home1\n",
    "3. After logging in go to Pivotal Web Services. See if you can get a free trail. I was not able to.\n",
    "<img src=\"images/image70.png\" align=\"middle\" style=\"width:800px;height:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create your own space named \"UAT\" or something.\n",
    "5. Once you go into \"UAT\" space you can choose to add services to it like any App monitoring services , Autoscaler, ClearDB MySQL Database etc\n",
    "6. Next is to Push teh code from local computer to Pivotla Cloud. AFter which Pivotal will provide you with a website. Do push the code download and install \"Cloud Factory Command Line Interface\" (search for cf cli download)\n",
    "7. https://github.com/cloudfoundry/cli , https://github.com/cloudfoundry/cli/blob/master/doc/installation-instructions/installation-instructions-v7.md look for windows installer and install the cli.\n",
    "8. make sure when you type >>cf or >>cf7 it shows some message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Next few lines of code change in the main python file app.py/flaskblog.py file.\n",
    "    * comment out app.run(debug=True)\n",
    "    * add these lines: \n",
    "    \n",
    "    port = int(os.getenv(\"PORT\"))  \n",
    "    if __name__ == '__main__':   \n",
    "    app.run(host='0.0.0.0', port=port)\n",
    "10. Make sure you have requirements.txt and is upto date.\n",
    "11. Create a file named runtime.txt (top most level , same folder where requirements.txt is present) with content 'python-3.6.9' (plain text without quotes).\n",
    "12. Create another file names 'Procfile' (top level same as above), with content   \n",
    "'web: python flask_app.py --master --processes 4 --threads 2'  flask_app.py is your main application python file.\n",
    "13. Create another file 'manifest.yml'(top level) with content -  \n",
    "    '---\n",
    "    application:  \n",
    "    -name: VeenaReviewScrapper  \n",
    "    memory: 1.5GB  \n",
    "    dish_quote: 1.5GB  \n",
    "    random-route: true  \n",
    "    parameters:  \n",
    "        memory: 1.5GB\n",
    "        buildpack: 'python_buildpack'\n",
    "    '  \n",
    "Just the baove without the quotes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. On your project terminal in Pycharm\n",
    "    * <>cf login -a https://api.run.pivotal.io\n",
    "    enter your login credentials here.\n",
    "15. Once its logs inn you will be select the space by typing 1,2 etc according to the space number.\n",
    "16. <>cf push \n",
    "17. This will push the code and make your app up and running in the cloud. Hopefully you will not get a fail message and everything works with you and you will get a success message will will say requested status: started... copy the route and paste it in a chrome browser.\n",
    "18. Then go to Pivotal page and check the status of your app which will be shown as \"running\" and explore more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Image Scraping with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the above scenarioe where we did web scraping of some text, whats happening is we are using request.get('url'). Here we are making a request to the server (say corey's website server or flipkart server) which is returning a HTML to us in response. And we are able to use BeautifulSoup + Parser to parse the HTML and filter out the tags we are interested.\n",
    "2. In another kind of web framework, some servers repond with HTML+JS(javascript) code. The JS code is then converted to HTML format by the browser in real time. \n",
    "3. Since in our Python Scraping we dont use any browser and to deal with a server response whicd is JS we will have to mimic the broswer. __Selenium Webdriver__ is one of the framework which will can do this job of converting the JS to HTML format.\n",
    "4. Python comes with a Selenium webdriver. But for it to work it still needs a web browser. So download the chrome driver and place in your project folder \"chromedriver.exe\" and make sure to double click and keep it running while the selenium web driver is in use. \n",
    "5. Then use the selenium webdriver to get teh job done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful links:\n",
    "    1. https://stackoverflow.com/questions/54962869/function-parameter-with-colon\n",
    "    2. https://stenevang.wordpress.com/2013/02/22/google-advanced-power-search-url-request-parameters/\n",
    "    3. https://www.youtube.com/watch?v=zucvHSQsKHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Scraping with just BS (no Selenium) and the problem associated with it: https://github.com/VeenaKGit/ImageScraper/releases\n",
    "1. When you do a image search say \"https://www.google.com/search?tbm=isch&q=cat\" like this one. Right click any of the image and see. You will find all over the place tags or atttrituted in HTML tags like 'jsname','jsaction', /</script/>/ etc. All those indicate that the HTML page contains lots of JavaScript and DOM (Document Object Model). Which is a first indicator that BS alone cannot be used for this kind of webscraping.\n",
    "2. You can still extract the images on that particular webpage using BS alone. I did it and it is the first version on git of ImageScraper - \"Version 1.0-lower resolution image download with BS alone\". But here is the problem. The downloaded images are of extreme low resolution (probabaly from a local google server storage which hold a low resolution copy of the image). So when you download the images source tag url (on the search page), you are not actually downloading the image from the actual website hosting the image in good resolution you are downloading it from google storage rather. __This is the first problem.__\n",
    "3. To get the original image in high resolution. Click the image once - this causes the JavaScript to open a small div on right which will actually load the image from its original website. Not right click and check the source of image. The image tag will now have the src='' which points to the actual high resolution image from teh original website. This is what we are supposed to download. But for this we will have to click the image once which is not possibel on BeautifulSoup. We would need Selenium webdriver for this task.\n",
    "4. With only BS you can download only limited low resolution images. beacuse scrolling down is down using JS. __This is the second problem.__ Check te answer below. https://stackoverflow.com/questions/21006940/how-to-load-all-entries-in-an-infinite-scroll-at-once-to-parse-the-html-in-pytho. So we have to use either Selenium or Scrapy here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Scraping with Selenium :\n",
    "1. Below is the whole code thats required for adding selenium to extract the good resolution images.\n",
    "2. So place the chromedriver.exe in the project path. Next make a instance of the webdriver \"with webdriver.Chrome()..\". Then load the page with wb. This is like opening the aearch page in chrome window. Actually a window will open which you can monitor.\n",
    "3. The collect the required amount(limit) of thumbnails with 'wd.find_elements_by_css_selector(\"img.Q4LuWd\")' this will make sure we are targetting the right image we want and not any adv or info images.\n",
    "4. Then click on each thumbnail and get the 'src' for the actual image per the img.img.n3VNCb. You may have multiple tags with smae class as img.n3VNCb ..so better look for the right one.\n",
    "5. This is pretty simple and we can load as much as 500 images at once which is not possible using BS alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def search():\n",
    "    if request.method == 'POST':\n",
    "        global search_string, img_urls, limit\n",
    "        img_urls.clear()\n",
    "        search_string = request.form['searchWord'].replace(\" \", \"+\")\n",
    "        limit = int(request.form['limit'].replace(\" \", \"\")) if request.form['limit'].replace(\" \", \"\") else 10\n",
    "\n",
    "        thumbnail_results = []\n",
    "        counter = 0\n",
    "        with webdriver.Chrome(executable_path=DRIVER_PATH) as wd:\n",
    "            source = wd.get(search_url.format(q=search_string))\n",
    "            while len(thumbnail_results) < (limit + buffer):\n",
    "                thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "                scroll_to_end(wd)\n",
    "\n",
    "            for img in thumbnail_results:\n",
    "                if counter <= limit:\n",
    "                    try:\n",
    "                        img.click()\n",
    "                        time.sleep(2)\n",
    "                    except Exception as e:\n",
    "                        print(\"Count not click the ThumbNail {}\".format(e))\n",
    "                        continue\n",
    "                    actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "\n",
    "                    for image in actual_images:\n",
    "                        if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "                            print('image {}- {}'.format(counter, image.get_attribute('src')))\n",
    "                            counter += 1\n",
    "                            img_urls.append(image.get_attribute('src'))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return render_template('home.html', images=img_urls,\n",
    "                           alt_name=search_string, button=True, msg='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating MongoDB with Image Scraper :\n",
    "1. Check on the docs to install and run MongoDB software.\n",
    "2. Check version 3.0 commit - cafcbe3b5abce55a04406c71a152c8a9b6a00578 for code changes to integrate theh MongoDB\n",
    "3. Made changes to add search results to database. Read the results from databse if already exists else make new search.\n",
    "4. Download is done from urls stored in database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Heroku:\n",
    "*. Only use deploy branch on ImageScraper Repository to deploy on heroku. It has code changes works only for deployment.\n",
    "1. Download the CLI from -https://devcenter.heroku.com/articles/heroku-cli and install. \n",
    "2. created a account in Heorku site. Use the same. Upto 5 free application\n",
    "3. make sure to add .idea/ to gitignore.\n",
    "4. Add a file called ‘Procfile’ inside the project folder. This folder contains the command to run the flask application once deployed on the server. Content: \"web : unicorn app:app \" - Here, the keyword ‘web’ specifies that the application is a web application. And the part ‘app:app’ instructs the program to look for a flask application called ‘app’ inside the ‘app.py’ file. Gunicorn is a Web Server Gateway Interface (WSGI) HTTP server for Python.\n",
    "5. Make sure you have requirements.txt and is up to date.\n",
    "6. appy.py should have only \"app.run(debug=Ture)\" in /_/_main__ thats it.\n",
    "7. commit all your changes to git.\n",
    "8. under your project directory in terminal type \"> heroku login\". Do per the instructions. Once logged in enter \">heroku create\" to create a heroku app. It will give you the URL of your Heroku app after successful creation. Check you heroku online for the new app created.\n",
    "https://devcenter.heroku.com/articles/git#creating-a-heroku-remote  \n",
    "9. enter \"git remote -v\" to see that you have 2 origins now. one is your github account one is heroku.\n",
    "10. Final step will be enter \"> git push heroku master\" . To deploy your app to Heroku, you typically use the git push command to push the code from your local repository’s master or main branch to your heroku remote.Use this same command whenever you want to deploy the latest committed version of your code to Heroku.Note that Heroku only deploys code that you push to master or main. Pushing code to another branch of the heroku remote has no effect.\n",
    "11. Mongo databse will not work in this case. We will need a mlab MongoDB resource addon free which is now discontinued. So better use the app version which was runnin without the database.\n",
    "12. to stop teh app - heroku dyno:kill DYNO - https://devcenter.heroku.com/articles/heroku-cli-commands\n",
    "13. There will some issue to run chromedriver for selenium ..so check this video. https://www.youtube.com/watch?v=Ven-pqwk3ec or https://www.andressevilla.com/running-chromedriver-with-python-selenium-on-heroku/\n",
    "14. Currently I stopped developing teh app...it has following major issue.. TImeout on server if search limit is more than 10 and cannot download images to client location. Heroku PostGres is free so try adding it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
