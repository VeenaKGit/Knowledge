{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark \n",
    "* https://www.youtube.com/watch?v=cYL42BBL3Fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course: Big Data Analytics using Spark\n",
    "* Course-->Introduction and Course Information-->Installing Software for the course and online workbench tutorial-->Software Installation Directions\n",
    "* The above course path will give you steps to install spark using Docker\n",
    "* Docker - Docker is a set of platform as a service products that uses OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Latency:\n",
    "1. General computation latency for a multiplication involves, moving A and B from memory to regists in CPU. Then pereforming computation, then moving back the result to memory.  \n",
    "<img src=\"images/image41.png\" align=\"middle\" style=\"width:200p ;height:200px\" />  \n",
    "2. In analyzing Big Data the latency of read and write operations dominate the computational latecy.\n",
    "3. Different types of storage offer different latency, capacity and price.\n",
    "4. Big data analytics revolves around methods for organising storage and computation in ways that maximize speed while minimizing cost.\n",
    "5. Depending upon budget we have to trade between small-fast memory and large-slow memory. Another option is using Cache. \n",
    "<img src=\"images/image42.png\" align=\"middle\" style=\"width:200p ;height:200px\" /> \n",
    "6. Every time a CPU looks for a memory location ex. 67 and its found in Cache then its a Cache hit and if CPU looks for a value which is not in Cache its a Cache miss(because it involves additional work of emptying cache space and getting the value from memory into cache). We need Cache hit rate to be high.\n",
    "7. Temporal locality: Multiple accesses to same address within a short time period.\n",
    "8. Spatial localoty: Multiple accesses to close-together(blocks) addresses in short time period. Since memory is parttioned into blocks/lines than single bytes. Transfering blocks is faster than chuck of data in different locations. And memory locations close to each other are more likely to fall in the same block and thus increase the hit rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "1. Spark, defined by its creators is a fast and general engine for large-scale data processing.\n",
    "2. The fast part means that itâ€™s faster than previous approaches to work with Big Data like classical MapReduce. The secret for being faster is that Spark runs on Memory (RAM), and that makes the processing much faster than on Disk.\n",
    "3. Spark can be deployed using Mesos, Hadoop via YARN or sparks own cluster manager.\n",
    "4. Spark provides high level API's in Java, Scala, Python and R. SPark code can be written in any of the four laungages.It also provides a shell in Scala and Python.\n",
    "5. Spark is basically written in Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Application Architecture:\n",
    "1. Storage: Spark supports (Distributed file system). Data can be stored in HDFS, Local FS, Amazon S3,  RDBMS,  NoSQL.\n",
    "2. Deployment Management : For this Spark has **Mesos, Yarn and Spark Cluster Manager**.\n",
    "3. Spark Core Execution Engine: Responsible for Spark Core functionality like I/O, scheduling, monitoring etc. Provids extensive API's\n",
    "4. Spark Library : Consists of Sprak SQL, Spark Straming, Spark MLlib, Spark GraphX\n",
    "5. Programming Language: Java, Python, Scala and R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Ecosystem and Other services:\n",
    "<img src=\"images/image152.png\" align=\"middle\" style=\"width:400p ;height:400px\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with other Hadoop system and introduction to RDD:\n",
    "1. For distributed computation over several jobs. Data needs to be divided, processed and stored. Intermediate data where stores and accessed on HDFS(Hard disk) which add up to lots of latency. Hadoop system had serial operations, meaning every iteration to fetch data entire data used to be scanned. Which Spark solved it by using in Memory data sharing and fault tolerant distributed in memory coputaion using RDD \n",
    "2. **RDD - Resilent Distributed Datasets** - They are the fundamental data structure of Spark. RDD's represent a collection of items distributed across many compute nodes that can be manipulated in parallel. They are schemaless structure that can handle both structured and unstructured data.\n",
    "3. RDD's are immutable. Meaning if you alter them, you create new RDD's\n",
    "4. Anything you do in sprak is around RDD, When you bring data into Spark, its read into RDD. When you perform transformations on data, you perform on old RDD to create new RDD. Later the data is stored from a RDD to persistent storage.\n",
    "5. RDDs can contain any data structure from Python, Java, Scala or R  objects, even user defined objects.\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
